{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fed7c516",
   "metadata": {},
   "source": [
    "## Vector Store DB\n",
    "1. FAISS is a library for efficient similarity search and clustering of dense vectors.\n",
    "\n",
    "2. It is particularly useful for large datasets and high-dimensional spaces.\n",
    "\n",
    "3. FAISS is a C++ library with bindings for Python, and it is designed to handle large-scale vector search problems efficiently.\n",
    "\n",
    "4. FAISS is widely used in applications such as image retrieval, recommendation systems, and natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a572a87",
   "metadata": {},
   "source": [
    "## ðŸ§  LangChain: Retrieval Chain & Document Chain Explained\n",
    "\n",
    "This guide explains two key LangChain components used to build RAG (Retrieval-Augmented Generation) systems: the **Document Chain** and the **Retrieval Chain**, using `create_stuff_documents_chain` and `ChatPromptTemplate`.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ What Is a Document Chain?\n",
    "\n",
    "A **Document Chain** is responsible for:\n",
    "- Taking retrieved documents (chunks).\n",
    "- Merging or formatting them into a single prompt.\n",
    "- Sending that prompt to the LLM for answering.\n",
    "\n",
    "### ðŸ”§ Example: Using `create_stuff_documents_chain`\n",
    "\n",
    "```python\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe89b4",
   "metadata": {},
   "source": [
    "### ðŸ§© Components:\n",
    "\n",
    "#### âœ… `ChatPromptTemplate`\n",
    "\n",
    "Used to define a custom prompt format for the LLM. You embed the document context and the user question here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb20801",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11971f03",
   "metadata": {},
   "source": [
    "-   `{context}` will be replaced by the combined document chunks.\n",
    "    \n",
    "-   This enforces that the LLM only uses the retrieved documents (not outside knowledge).\n",
    "    \n",
    "\n",
    "#### âœ… `create_stuff_documents_chain`\n",
    "\n",
    "This creates a **\"stuff\" document chain**, which:\n",
    "\n",
    "-   Takes all document chunks.\n",
    "    \n",
    "-   \"Stuffs\" them into the prompt as a big context block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ddf643",
   "metadata": {},
   "source": [
    "## ðŸ”„ What About the Retrieval Chain?\n",
    "\n",
    "A **Retrieval Chain** combines:\n",
    "\n",
    "-   A **retriever** (from a vector store).\n",
    "    \n",
    "-   A **document chain** (like the one above).\n",
    "    \n",
    "-   Together, they retrieve relevant context and generate responses.\n",
    "    \n",
    "\n",
    "### Typical Structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb57d52",
   "metadata": {},
   "source": [
    "```bash\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retrieval_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a6f7e9",
   "metadata": {},
   "source": [
    "Alternatively, you can plug in your own custom document_chain like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babe6c99",
   "metadata": {},
   "source": [
    "```bash\n",
    "retrieval_chain = RetrievalQA(retriever=retriever, combine_documents_chain=document_chain)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9f721",
   "metadata": {},
   "source": [
    "## âœ… Usage in a RAG Workflow\n",
    "\n",
    "1.  **Ingest & Embed documents** into a vector store (e.g. FAISS).\n",
    "    \n",
    "2.  Use the **retriever** to fetch top-k chunks.\n",
    "    \n",
    "3.  Format them with a **custom prompt** using `ChatPromptTemplate`.\n",
    "    \n",
    "4.  Feed them into a **document chain** with `create_stuff_documents_chain`.\n",
    "    \n",
    "5.  Wrap it in a **RetrievalQA chain** to handle queries end-to-end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6737891c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
