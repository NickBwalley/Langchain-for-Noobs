{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e20afc20",
   "metadata": {},
   "source": [
    "## ðŸ“˜ What is LCEL?\n",
    "\n",
    "**LCEL (LangChain Expression Language)** is a declarative, modular framework inside LangChain for building **chains** (i.e., data processing pipelines).\n",
    "\n",
    "Instead of imperatively writing long functions, LCEL lets you **compose chains** using simple `|` syntax (like Unix pipes), which:\n",
    "\n",
    "-   Passes data from one component to the next.\n",
    "    \n",
    "-   Keeps your logic clean, readable, and reusable.\n",
    "\n",
    "## ðŸ§  Why Was LCEL Created?\n",
    "\n",
    "### âœ… Problems it Solves:\n",
    "\n",
    "Before LCEL, LangChain chains were created like this:\n",
    "```bash\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=my_prompt)\n",
    "response = chain.run(\"your question here\")\n",
    "```\n",
    "While this worked, it became hard to:\n",
    "\n",
    "-   **Customize components** (like switching prompts or adding memory).\n",
    "    \n",
    "-   **Test/debug subcomponents**.\n",
    "    \n",
    "-   **Combine multiple tools** (e.g., retriever + summarizer + formatter).\n",
    "    \n",
    "-   **Chain arbitrary logic** declaratively.\n",
    "\n",
    "\n",
    "### âœ… LCEL Solves That By:\n",
    "\n",
    "-   Making chains **flexible and modular**.\n",
    "    \n",
    "-   Using simple syntax like:\n",
    "    \n",
    "    `chain = retriever | summarizer | output_parser`\n",
    "    \n",
    "-   Each part does **one task**, and can be composed or reused easily.\n",
    "\n",
    "## ðŸš€ When Should You Use LCEL?\n",
    "\n",
    "-   When building **custom RAG pipelines**.\n",
    "    \n",
    "-   When you want **clear, testable steps** in a chain.\n",
    "    \n",
    "-   When combining **retrieval, generation, formatting, and tools**.\n",
    "    \n",
    "-   When working with **Async APIs** (LCEL supports async execution out of the box).\n",
    "\n",
    "\n",
    "## ðŸ“¦ LCEL Core Components\n",
    "\n",
    "| Component        | Purpose                                 |\n",
    "|------------------|-----------------------------------------|\n",
    "| `Runnable`       | The base interface (can be chained).    |\n",
    "| `RunnableMap`    | For branching/merging data streams.     |\n",
    "| `RunnableLambda` | Wrap a function into an LCEL step.      |\n",
    "| `RunnableSequence` | Manual sequence of LCEL components.     |\n",
    "| `|` Operator     | Chain operations (pipe style).          |\n",
    "\n",
    "## ðŸ§ª LCEL in Action â€” Example App: Summarize User Input\n",
    "\n",
    "Let's now build a simple LCEL pipeline:\n",
    "\n",
    "-   User provides a question.\n",
    "    \n",
    "-   We format it with a prompt.\n",
    "    \n",
    "-   It goes through an LLM.\n",
    "    \n",
    "-   Output is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6699c2d7",
   "metadata": {},
   "source": [
    "## âœ… Step-by-Step LCEL Example (With Inline Comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cda7616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: LangChain Expression Language simplifies the process of creating intricate pipelines by combining basic elements such as prompts, LLMs, and retrievers.\n"
     ]
    }
   ],
   "source": [
    "# Import core components from LangChain\n",
    "from langchain.chat_models import ChatOpenAI  # LLM\n",
    "from langchain_core.prompts import ChatPromptTemplate  # Prompt builder\n",
    "from langchain_core.output_parsers import StrOutputParser  # To convert LLM response to string\n",
    "\n",
    "# 1. Load your LLM model (e.g., OpenAI GPT-3.5)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# 2. Create a chat prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize the following input in one short sentence:\\n\\n{user_input}\"\n",
    ")\n",
    "\n",
    "# 3. Set up an output parser to turn the raw LLM response into a clean string\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 4. Compose the LCEL chain using the pipe (|) operator\n",
    "#    Step 1: user_input -> prompt -> LLM -> parsed output\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# 5. Run the chain with some sample input\n",
    "user_input = {\"user_input\": \"LangChain Expression Language helps you build complex pipelines by composing simple building blocks like prompts, LLMs, and retrievers.\"}\n",
    "\n",
    "# 6. Call the chain and get the result\n",
    "result = chain.invoke(user_input)\n",
    "\n",
    "# 7. Print the summarized result\n",
    "print(\"Summary:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8ed37e",
   "metadata": {},
   "source": [
    "## âœ… Summary\n",
    "\n",
    "| Feature | Description |\n",
    "|--------|-------------|\n",
    "| **What** | LCEL is a composable, declarative way to build chains in LangChain. |\n",
    "| **Why** | It simplifies debugging, testing, and customization of steps. |\n",
    "| **Use When** | You need custom RAG apps, complex chains, or async-friendly components. |\n",
    "| **Core Syntax** | `prompt | llm | parser` is the new pattern. |\n",
    "| **Benefit** | Modular, readable, and future-proof pipelines. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6922f68",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
